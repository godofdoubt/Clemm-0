
WARP_DRIVE_KEY=YourSecureKey
# .env

# .env

# --- Paths --
# Correct path to the GGUF model file.
# A relative path is clean and works since the script runs from the project root.
RAVEN_GGUF_MODEL_PATH=models/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf
#Qwen3-22B-A3B-The-Harley-Quinn-PUDDIN-Abliterated-Uncensored.i1-Q4_K_M.gguf
#DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf
#RoboBrain2.0-7B-Q6_K_L.gguf
#DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf
#qwen3-8b-64k-context-2x-josiefied-uncensored-q6_k.gguf
# Correct path to the llama-server executable, based on your own command line output.
# Use forward slashes for consistency.
LLAMACPP_SERVER_EXECUTABLE_PATH=C:/Users/Mert/Desktop/CLEMM/LLAMASERVERCUDA/llama.cpp/build/bin/Release/llama-server.exe




# --- Server Backend Settings ---
# URL for the server. The script will start the server at this address.
LLAMACPP_SERVER_URL=http://127.0.0.1:8080
SERVER_CONTEXT_SIZE=1200
# Number of GPU layers for the server to offload.
# You confirmed `297b` 27 22b works well for your GPU. 33 for dpseekq3
SERVER_GPU_LAYERS=34

# --- CUDA Backend Settings (for direct library use) ---
CONTEXT_SIZE=1500
# Number of CPU threads for the direct CUDA backend.
CPU_THREADS=6