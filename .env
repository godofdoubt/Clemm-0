# Correct path to the GGUF model file.
#.env in main folder
RAVEN_GGUF_MODEL_PATH=models/Q8_0.gguf
WARP_DRIVE_KEY=YourSecureKey
LLAMACPP_SERVER_EXECUTABLE_PATH=C:/Users/Mert/Desktop/CLEMM/LLAMASERVERCUDA/llama.cpp/build/bin/Release/llama-server.exe
# Correct server path file.
LLAMACPP_SERVER_URL=http://127.0.0.1:8080
SERVER_CONTEXT_SIZE=1200
SERVER_GPU_LAYERS=34
# --- CUDA Backend Settings (for direct library use) ---
CONTEXT_SIZE=1500
# Number of CPU threads for the direct CUDA backend.
CPU_THREADS=6
# --- Paths --
